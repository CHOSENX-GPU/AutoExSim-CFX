"""
å·¥ä½œæµç¼–æ’æ¨¡å—
åè°ƒæ•´ä¸ªCFXè‡ªåŠ¨åŒ–æµç¨‹çš„æ‰§è¡Œ
"""

import logging
import os
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import paramiko

from .config import CFXAutomationConfig
from .cfx import CFXManager
from .cluster_query import ClusterQueryManager
from .script_generator import ScriptGenerator
from .transfer import FileTransferManager
from .job_monitor import JobMonitor


class WorkflowError(Exception):
    """å·¥ä½œæµæ‰§è¡Œé”™è¯¯"""
    pass


class WorkflowOrchestrator:
    """å·¥ä½œæµç¼–æ’å™¨"""
    
    def __init__(self, config: CFXAutomationConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.cfx_manager = CFXManager(config)
        self.cluster_query = ClusterQueryManager(config) if config.enable_node_detection else None
        self.script_generator = ScriptGenerator(config)
        self.transfer_manager = FileTransferManager(config)
        self.job_monitor = JobMonitor(config)
        
        # SSHè¿æ¥
        self.ssh_client = None
        
        # æ‰§è¡ŒçŠ¶æ€
        self.execution_state = {
            "current_step": "",
            "completed_steps": [],
            "failed_steps": [],
            "start_time": None,
            "end_time": None,
            "total_jobs": 0,
            "successful_jobs": 0
        }
    
    def execute_full_workflow(self, job_configs: List[Dict]) -> Dict:
        """
        æ‰§è¡Œå®Œæ•´çš„å·¥ä½œæµç¨‹
        
        Args:
            job_configs: ä½œä¸šé…ç½®åˆ—è¡¨
            
        Returns:
            Dict: æ‰§è¡Œç»“æœæŠ¥å‘Š
        """
        self.logger.info("å¼€å§‹æ‰§è¡ŒCFXè‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹...")
        
        try:
            # åˆå§‹åŒ–æ‰§è¡ŒçŠ¶æ€
            self._initialize_execution_state(job_configs)
            
            # æ­¥éª¤1: è¿æ¥æœåŠ¡å™¨
            self._execute_step("connect_server", self._connect_to_server)
            
            # æ­¥éª¤2: éªŒè¯CFXç¯å¢ƒ
            self._execute_step("verify_cfx", self._verify_cfx_environment)
            
            # æ­¥éª¤3: ç”Ÿæˆ.preæ–‡ä»¶
            pre_files = self._execute_step("generate_pre", 
                                         lambda: self.cfx_manager.generate_pre_files(job_configs))
            
            # æ­¥éª¤4: ç”Ÿæˆ.defæ–‡ä»¶ï¼ˆæ ¹æ®æ¨¡å¼ï¼‰
            def_files = self._execute_step("generate_def", 
                                         lambda: self._generate_def_files(pre_files, job_configs))
            
            # æ­¥éª¤5: é›†ç¾¤èŠ‚ç‚¹æŸ¥è¯¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            cluster_status = None
            if self.config.enable_node_detection:
                cluster_status = self._execute_step("query_cluster", 
                                                  lambda: self.cluster_query.query_cluster_nodes(self.ssh_client))
            
            # æ­¥éª¤6: ç”Ÿæˆä½œä¸šè„šæœ¬ï¼ˆåŒ…å«æ™ºèƒ½èŠ‚ç‚¹åˆ†é…ï¼‰
            # ä½¿ç”¨ç®€åŒ–çš„ä½œä¸šé…ç½®ï¼Œæ™ºèƒ½åˆ†é…åœ¨è„šæœ¬ç”Ÿæˆæ—¶è¿›è¡Œ
            simple_jobs = self._create_simple_job_configs(job_configs, def_files)
            scripts = self._execute_step("generate_scripts", 
                                       lambda: self.script_generator.generate_job_scripts(simple_jobs, cluster_status))
            
            # æ­¥éª¤8: ä¸Šä¼ æ–‡ä»¶
            self._execute_step("upload_files", 
                             lambda: self._upload_files(def_files, scripts))
            
            # æ­¥éª¤9: æäº¤ä½œä¸š
            submitted_jobs = self._execute_step("submit_jobs", 
                                              lambda: self._submit_jobs(scripts["submit_script"]))
            
            # æ­¥éª¤10: ç›‘æ§ä½œä¸šï¼ˆå¦‚æœå¯ç”¨ï¼‰
            monitoring_report = None
            if self.config.enable_monitoring:
                monitoring_report = self._execute_step("monitor_jobs", 
                                                     lambda: self._monitor_jobs(submitted_jobs))
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            report = self._generate_final_report(
                job_configs, simple_jobs, scripts, monitoring_report
            )
            
            self.execution_state["end_time"] = datetime.now().isoformat()
            self.logger.info("CFXè‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆ")
            
            return report
            
        except Exception as e:
            self.logger.error(f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            self.execution_state["end_time"] = datetime.now().isoformat()
            raise WorkflowError(f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
        
        finally:
            # æ¸…ç†èµ„æº
            self._cleanup_resources()
    
    def _initialize_execution_state(self, job_configs: List[Dict]) -> None:
        """åˆå§‹åŒ–æ‰§è¡ŒçŠ¶æ€"""
        self.execution_state.update({
            "current_step": "",
            "completed_steps": [],
            "failed_steps": [],
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "total_jobs": len(job_configs),
            "successful_jobs": 0
        })
    
    def _execute_step(self, step_name: str, step_function) -> any:
        """æ‰§è¡Œå·¥ä½œæµæ­¥éª¤"""
        self.execution_state["current_step"] = step_name
        self.logger.info(f"æ‰§è¡Œæ­¥éª¤: {step_name}")
        
        try:
            result = step_function()
            self.execution_state["completed_steps"].append(step_name)
            self.logger.info(f"æ­¥éª¤å®Œæˆ: {step_name}")
            return result
            
        except Exception as e:
            self.execution_state["failed_steps"].append(step_name)
            self.logger.error(f"æ­¥éª¤å¤±è´¥ {step_name}: {e}")
            raise WorkflowError(f"æ­¥éª¤å¤±è´¥ {step_name}: {e}")
    
    def _connect_to_server(self) -> None:
        """è¿æ¥åˆ°æœåŠ¡å™¨"""
        try:
            self.ssh_client = paramiko.SSHClient()
            self.ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            
            # è¿æ¥å‚æ•°
            connect_kwargs = {
                "hostname": self.config.ssh_host,
                "port": self.config.ssh_port,
                "username": self.config.ssh_user,
                "timeout": 30
            }
            
            # è®¤è¯æ–¹å¼
            if self.config.ssh_key and os.path.exists(self.config.ssh_key):
                connect_kwargs["key_filename"] = self.config.ssh_key
            elif self.config.ssh_password:
                connect_kwargs["password"] = self.config.ssh_password
            else:
                raise WorkflowError("æœªé…ç½®SSHè®¤è¯ä¿¡æ¯")
            
            self.ssh_client.connect(**connect_kwargs)
            self.logger.info(f"å·²è¿æ¥åˆ°æœåŠ¡å™¨: {self.config.ssh_host}")
            
        except Exception as e:
            raise WorkflowError(f"æœåŠ¡å™¨è¿æ¥å¤±è´¥: {e}")
    
    def _verify_cfx_environment(self) -> None:
        """éªŒè¯CFXç¯å¢ƒ"""
        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†è·³è¿‡CFXéªŒè¯ï¼ˆè€é›†ç¾¤ä½¿ç”¨module systemï¼‰
        skip_verification = getattr(self.config, 'skip_cfx_verification', False)
        self.logger.info(f"CFXéªŒè¯è®¾ç½®: skip_cfx_verification = {skip_verification}")
        
        if skip_verification:
            self.logger.info("è·³è¿‡æœåŠ¡å™¨CFXç¯å¢ƒéªŒè¯ (è€é›†ç¾¤ä½¿ç”¨module system)")
            # å¦‚æœæ˜¯localæ¨¡å¼ï¼Œä»éœ€éªŒè¯æœ¬åœ°CFXç¯å¢ƒæ˜¯å¦å¯ç”¨
            if self.config.cfx_mode == "local":
                if not self.config.cfx_pre_executable:
                    self.logger.error("æœ¬åœ°CFXç¯å¢ƒæœªé…ç½®: cfx_pre_executableä¸ºç©º")
                    raise WorkflowError("æœ¬åœ°CFXç¯å¢ƒæœªé…ç½®æˆ–ä¸å¯ç”¨")
                else:
                    self.logger.info(f"æœ¬åœ°CFXç¯å¢ƒå·²é…ç½®: {self.config.cfx_pre_executable}")
            return
            
        # æ­£å¸¸éªŒè¯æœåŠ¡å™¨CFXç¯å¢ƒ
        self.logger.info("æ‰§è¡Œæ­£å¸¸çš„æœåŠ¡å™¨CFXç¯å¢ƒéªŒè¯")
        if not self.cfx_manager.verify_server_cfx_environment(self.ssh_client):
            raise WorkflowError("æœåŠ¡å™¨CFXç¯å¢ƒéªŒè¯å¤±è´¥")
        
        # å¦‚æœæ˜¯localæ¨¡å¼ï¼ŒéªŒè¯æœ¬åœ°CFXç¯å¢ƒ
        if self.config.cfx_mode == "local":
            if not self.config.cfx_pre_executable:
                raise WorkflowError("æœ¬åœ°CFXç¯å¢ƒæœªé…ç½®æˆ–ä¸å¯ç”¨")
    
    def _generate_def_files(self, pre_files: List[str], job_configs: List[Dict]) -> List[str]:
        """ç”Ÿæˆ.defæ–‡ä»¶"""
        if self.config.cfx_mode == "local":
            # æœ¬åœ°ç”Ÿæˆ.defæ–‡ä»¶
            return self.cfx_manager.generate_def_files_local(pre_files)
        else:
            # æœåŠ¡å™¨ç”Ÿæˆ.defæ–‡ä»¶
            # é¦–å…ˆä¸Šä¼ .preæ–‡ä»¶
            remote_pre_files = self.transfer_manager.upload_files(
                self.ssh_client, pre_files, self.config.remote_base_path
            )
            
            # å‡†å¤‡æœåŠ¡å™¨CFXç¯å¢ƒ
            self.cfx_manager.prepare_server_cfx_generation(
                self.ssh_client, pre_files, self.config.remote_base_path
            )
            
            # åœ¨æœåŠ¡å™¨ç”Ÿæˆ.defæ–‡ä»¶
            return self.cfx_manager.generate_def_files_server(
                self.ssh_client, list(remote_pre_files.values()), self.config.remote_base_path
            )
    
    def _create_simple_job_configs(self, job_configs: List[Dict], def_files: List[str]) -> List[Dict]:
        """åˆ›å»ºç®€åŒ–çš„ä½œä¸šé…ç½®ï¼ˆä¸è¿›è¡ŒèŠ‚ç‚¹åˆ†é…ï¼‰"""
        simple_jobs = []
        
        for i, job_config in enumerate(job_configs):
            simple_job = job_config.copy()
            simple_job.update({
                "def_file": def_files[i] if i < len(def_files) else "",
                "allocated_cpus": getattr(self.config, 'min_cores', self.config.tasks_per_node),
                "creation_time": datetime.now().isoformat()
            })
            simple_jobs.append(simple_job)
        
        return simple_jobs

    def _upload_files(self, def_files: List[str], scripts: Dict) -> None:
        """ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨"""
        files_to_upload = []
        
        # æ·»åŠ .defæ–‡ä»¶ï¼ˆå¦‚æœæ˜¯localæ¨¡å¼ï¼‰
        if self.config.cfx_mode == "local":
            files_to_upload.extend(def_files)
        
        # æ·»åŠ è„šæœ¬æ–‡ä»¶
        files_to_upload.extend(scripts["job_scripts"])
        if scripts["submit_script"]:
            files_to_upload.append(scripts["submit_script"])
        if scripts["monitor_script"]:
            files_to_upload.append(scripts["monitor_script"])
        
        # æ‰§è¡ŒåŸºæœ¬æ–‡ä»¶ä¸Šä¼ ï¼Œä¿æŒç›®å½•ç»“æ„
        uploaded_files = self.transfer_manager.upload_files(
            self.ssh_client, files_to_upload, self.config.remote_base_path, preserve_structure=True
        )
        
        # å•ç‹¬å¤„ç†åˆå§‹æ–‡ä»¶ä¸Šä¼ åˆ°å„ä¸ªP_Out_æ–‡ä»¶å¤¹
        if hasattr(self.config, 'initial_file') and self.config.initial_file:
            initial_file_path = self.config.initial_file
            if os.path.exists(initial_file_path):
                self.logger.info(f"å¼€å§‹ä¸Šä¼ åˆå§‹æ–‡ä»¶åˆ°å„ä¸ªP_Out_æ–‡ä»¶å¤¹: {initial_file_path}")
                self._upload_initial_files_to_folders(initial_file_path, def_files)
            else:
                self.logger.warning(f"åˆå§‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ : {initial_file_path}")
        
        self.logger.info(f"æ–‡ä»¶ä¸Šä¼ å®Œæˆ: {len(uploaded_files)}ä¸ªæ–‡ä»¶")
    
    def _upload_initial_files_to_folders(self, initial_file_path: str, def_files: List[str]) -> None:
        """ç›´æ¥ä¸Šä¼ åˆå§‹æ–‡ä»¶åˆ°æœåŠ¡å™¨çš„å„ä¸ªP_Out_æ–‡ä»¶å¤¹"""
        try:
            # è·å–åˆå§‹æ–‡ä»¶çš„æ–‡ä»¶å
            initial_filename = os.path.basename(initial_file_path)
            
            # ä¸ºæ¯ä¸ªdefæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹ä¸Šä¼ åˆå§‹æ–‡ä»¶
            processed_folders = set()
            
            for def_file in def_files:
                # è·å–defæ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
                rel_def_file = os.path.relpath(def_file, self.config.base_path)
                def_folder_rel = os.path.dirname(rel_def_file)
                
                # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªæ–‡ä»¶å¤¹
                if def_folder_rel in processed_folders:
                    continue
                processed_folders.add(def_folder_rel)
                
                # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦æ˜¯P_Out_å¼€å¤´
                folder_name = os.path.basename(def_folder_rel)
                if folder_name.startswith(self.config.folder_prefix):
                    # æ„å»ºæœåŠ¡å™¨ç«¯çš„ç›®æ ‡è·¯å¾„
                    remote_folder = os.path.join(self.config.remote_base_path, def_folder_rel).replace('\\', '/')
                    remote_initial_file = f"{remote_folder}/{initial_filename}"
                    
                    self.logger.info(f"ä¸Šä¼ åˆå§‹æ–‡ä»¶åˆ°: {remote_initial_file}")
                    
                    # ä½¿ç”¨SFTPç›´æ¥ä¸Šä¼ æ–‡ä»¶åˆ°ç›®æ ‡è·¯å¾„
                    try:
                        sftp = self.ssh_client.open_sftp()
                        
                        # ç¡®ä¿è¿œç¨‹ç›®å½•å­˜åœ¨
                        try:
                            sftp.stat(remote_folder)
                        except FileNotFoundError:
                            # ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºç›®å½•
                            self._create_remote_directory(sftp, remote_folder)
                        
                        # ä¸Šä¼ æ–‡ä»¶
                        sftp.put(initial_file_path, remote_initial_file)
                        self.logger.info(f"âœ“ åˆå§‹æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {folder_name}/{initial_filename}")
                        
                        sftp.close()
                        
                    except Exception as e:
                        self.logger.error(f"ä¸Šä¼ åˆå§‹æ–‡ä»¶åˆ° {folder_name} å¤±è´¥: {e}")
                        
        except Exception as e:
            self.logger.error(f"ä¸Šä¼ åˆå§‹æ–‡ä»¶åˆ°æ–‡ä»¶å¤¹å¤±è´¥: {e}")
    
    def _create_remote_directory(self, sftp, remote_path: str) -> None:
        """é€’å½’åˆ›å»ºè¿œç¨‹ç›®å½•"""
        try:
            sftp.stat(remote_path)
        except FileNotFoundError:
            # ç›®å½•ä¸å­˜åœ¨ï¼Œå…ˆåˆ›å»ºçˆ¶ç›®å½•
            parent_dir = os.path.dirname(remote_path)
            if parent_dir and parent_dir != remote_path:
                self._create_remote_directory(sftp, parent_dir)
            
            # åˆ›å»ºå½“å‰ç›®å½•
            sftp.mkdir(remote_path)
            self.logger.debug(f"åˆ›å»ºè¿œç¨‹ç›®å½•: {remote_path}")
    
    def _prepare_initial_files_for_folders(self, initial_file_path: str, def_files: List[str]) -> None:
        """ä¸ºæ¯ä¸ªP_Out_æ–‡ä»¶å¤¹å‡†å¤‡åˆå§‹æ–‡ä»¶å‰¯æœ¬"""
        try:
            import shutil
            
            # è·å–åˆå§‹æ–‡ä»¶çš„æ–‡ä»¶å
            initial_filename = os.path.basename(initial_file_path)
            
            # ä¸ºæ¯ä¸ªdefæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹åˆ›å»ºåˆå§‹æ–‡ä»¶å‰¯æœ¬
            processed_folders = set()
            
            for def_file in def_files:
                # è·å–defæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹
                def_folder = os.path.dirname(def_file)
                
                # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªæ–‡ä»¶å¤¹
                if def_folder in processed_folders:
                    continue
                processed_folders.add(def_folder)
                
                # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦æ˜¯P_Out_å¼€å¤´
                folder_name = os.path.basename(def_folder)
                if folder_name.startswith(self.config.folder_prefix):
                    # åœ¨è¯¥æ–‡ä»¶å¤¹ä¸­åˆ›å»ºåˆå§‹æ–‡ä»¶å‰¯æœ¬
                    target_initial_file = os.path.join(def_folder, initial_filename)
                    
                    if not os.path.exists(target_initial_file):
                        self.logger.info(f"å¤åˆ¶åˆå§‹æ–‡ä»¶åˆ°: {target_initial_file}")
                        shutil.copy2(initial_file_path, target_initial_file)
                    else:
                        self.logger.info(f"åˆå§‹æ–‡ä»¶å·²å­˜åœ¨: {target_initial_file}")
                        
        except Exception as e:
            self.logger.error(f"å‡†å¤‡åˆå§‹æ–‡ä»¶å‰¯æœ¬å¤±è´¥: {e}")
    
    def _submit_jobs(self, submit_script: str) -> List[Dict]:
        """æäº¤ä½œä¸šåˆ°é˜Ÿåˆ—"""
        if not submit_script:
            raise WorkflowError("æ‰¹é‡æäº¤è„šæœ¬æœªç”Ÿæˆ")
        
        try:
            # è·å–è¿œç¨‹æäº¤è„šæœ¬è·¯å¾„
            remote_script = os.path.join(
                self.config.remote_base_path, 
                os.path.basename(submit_script)
            ).replace('\\', '/')
            
            # è®¾ç½®æ‰§è¡Œæƒé™
            chmod_cmd = f"chmod +x {remote_script}"
            stdin, stdout, stderr = self.ssh_client.exec_command(chmod_cmd)
            stdout.channel.recv_exit_status()
            
            # æ‰§è¡Œæäº¤è„šæœ¬
            submit_cmd = f"cd {self.config.remote_base_path} && ./{os.path.basename(submit_script)}"
            self.logger.info(f"æ‰§è¡Œä½œä¸šæäº¤å‘½ä»¤: {submit_cmd}")
            
            stdin, stdout, stderr = self.ssh_client.exec_command(submit_cmd)
            exit_status = stdout.channel.recv_exit_status()
            
            if exit_status != 0:
                error_msg = stderr.read().decode()
                raise WorkflowError(f"ä½œä¸šæäº¤å¤±è´¥: {error_msg}")
            
            output = stdout.read().decode()
            self.logger.info("ä½œä¸šæäº¤è¾“å‡º:")
            self.logger.info(output)
            
            # è§£ææäº¤çš„ä½œä¸šID
            submitted_jobs = []
            for line in output.split('\n'):
                if 'Submitted batch job' in line:
                    # ä»ç±»ä¼¼ "Submitted batch job 11122885" çš„è¡Œä¸­æå–ä½œä¸šID
                    try:
                        job_id = line.strip().split()[-1]
                        job_info = {
                            "job_id": job_id,
                            "output_line": line.strip()
                        }
                        submitted_jobs.append(job_info)
                    except Exception as e:
                        self.logger.warning(f"è§£æä½œä¸šIDå¤±è´¥: {line.strip()} - {e}")
            
            self.logger.info(f"æˆåŠŸæäº¤{len(submitted_jobs)}ä¸ªä½œä¸š")
            return submitted_jobs
            
        except Exception as e:
            raise WorkflowError(f"ä½œä¸šæäº¤å¤±è´¥: {e}")
    
    def _get_submitted_jobs(self) -> List[Dict]:
        """è·å–å½“å‰ç”¨æˆ·å·²æäº¤çš„ä½œä¸šä¿¡æ¯"""
        try:
            # æŸ¥è¯¢å½“å‰ç”¨æˆ·çš„ä½œä¸š
            if self.config.scheduler_type == "SLURM":
                query_cmd = f"squeue -u {self.config.ssh_user} -o '%.10i %.12P %.20j %.8u %.2t %.10M %.6D %R'"
            else:
                # PBSè°ƒåº¦å™¨
                query_cmd = f"qstat -u {self.config.ssh_user}"
            
            stdin, stdout, stderr = self.ssh_client.exec_command(query_cmd)
            exit_status = stdout.channel.recv_exit_status()
            
            if exit_status != 0:
                error_msg = stderr.read().decode()
                self.logger.warning(f"æŸ¥è¯¢ä½œä¸šçŠ¶æ€å¤±è´¥: {error_msg}")
                return []
            
            output = stdout.read().decode()
            self.logger.info("å½“å‰ä½œä¸šçŠ¶æ€æŸ¥è¯¢ç»“æœ:")
            self.logger.info(output)
            
            # è§£æä½œä¸šä¿¡æ¯
            jobs = []
            lines = output.strip().split('\n')
            
            if len(lines) <= 1:  # åªæœ‰æ ‡é¢˜è¡Œæˆ–ç©ºè¾“å‡º
                return []
            
            # è·³è¿‡æ ‡é¢˜è¡Œ
            for line in lines[1:]:
                if line.strip():
                    fields = line.split()
                    if len(fields) >= 5:
                        job_info = {
                            "job_id": fields[0],
                            "partition": fields[1] if len(fields) > 1 else "",
                            "name": fields[2] if len(fields) > 2 else "",
                            "user": fields[3] if len(fields) > 3 else "",
                            "state": fields[4] if len(fields) > 4 else "",
                            "time": fields[5] if len(fields) > 5 else "",
                            "nodes": fields[6] if len(fields) > 6 else "",
                            "nodelist": ' '.join(fields[7:]) if len(fields) > 7 else ""
                        }
                        jobs.append(job_info)
            
            self.logger.info(f"æ‰¾åˆ° {len(jobs)} ä¸ªæ­£åœ¨è¿è¡Œæˆ–æ’é˜Ÿçš„ä½œä¸š")
            return jobs
            
        except Exception as e:
            self.logger.error(f"è·å–ä½œä¸šä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def _monitor_jobs(self, submitted_jobs: List[Dict]) -> Dict:
        """ç›‘æ§ä½œä¸šæ‰§è¡Œ"""
        if not self.config.enable_monitoring:
            self.logger.info("ä½œä¸šç›‘æ§å·²ç¦ç”¨")
            return {}
        
        try:
            # å¯åŠ¨ç›‘æ§
            self.job_monitor.start_monitoring(self.ssh_client, submitted_jobs)
            
            # æ‰§è¡Œç›‘æ§å¾ªç¯
            monitoring_report = self.job_monitor.monitor_jobs(
                self.ssh_client, self.transfer_manager
            )
            
            return monitoring_report
            
        except KeyboardInterrupt:
            self.logger.info("ç›‘æ§è¢«ç”¨æˆ·ä¸­æ–­")
            return self.job_monitor._generate_monitoring_report()
        except Exception as e:
            raise WorkflowError(f"ä½œä¸šç›‘æ§å¤±è´¥: {e}")
    
    def _generate_final_report(self, job_configs: List[Dict], allocated_jobs: List[Dict], 
                             scripts: Dict, monitoring_report: Optional[Dict]) -> Dict:
        """ç”Ÿæˆæœ€ç»ˆæ‰§è¡ŒæŠ¥å‘Š"""
        # è®¡ç®—æ‰§è¡Œæ—¶é•¿
        start_time = datetime.fromisoformat(self.execution_state["start_time"])
        end_time = datetime.fromisoformat(self.execution_state["end_time"]) if self.execution_state["end_time"] else datetime.now()
        execution_duration = int((end_time - start_time).total_seconds())
        
        report = {
            "execution_summary": {
                "total_jobs": len(job_configs),
                "successful_submissions": len(allocated_jobs) if allocated_jobs else 0,
                "execution_duration_seconds": execution_duration,
                "completed_steps": self.execution_state["completed_steps"],
                "failed_steps": self.execution_state["failed_steps"],
                "start_time": self.execution_state["start_time"],
                "end_time": self.execution_state["end_time"]
            },
            "configuration": {
                "cfx_mode": self.config.cfx_mode,
                "cluster_type": self.config.cluster_type,
                "scheduler_type": self.config.scheduler_type,
                "node_allocation_enabled": self.config.enable_node_allocation,
                "monitoring_enabled": self.config.enable_monitoring
            },
            "generated_files": {
                "job_scripts": scripts.get("job_scripts", []) if scripts else [],
                "submit_script": scripts.get("submit_script", "") if scripts else "",
                "monitor_script": scripts.get("monitor_script", "") if scripts else ""
            },
            "transfer_statistics": self.transfer_manager.get_transfer_statistics(),
            "monitoring_report": monitoring_report if monitoring_report else {},
            "report_generation_time": datetime.now().isoformat()
        }
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        self._save_execution_report(report)
        
        return report
    
    def _save_execution_report(self, report: Dict) -> None:
        """ä¿å­˜æ‰§è¡ŒæŠ¥å‘Š"""
        try:
            import json
            
            # åˆ›å»ºreportç›®å½• - ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•è€Œä¸æ˜¯config.base_path
            current_dir = os.getcwd()
            report_dir = os.path.join(current_dir, "report")
            os.makedirs(report_dir, exist_ok=True)
            
            report_file = os.path.join(
                report_dir,
                f"cfx_execution_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"æ‰§è¡ŒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ‰§è¡ŒæŠ¥å‘Šå¤±è´¥: {e}")
    
    def _generate_step_report(self, step_name: str, result) -> None:
        """ä¸ºå•ç‹¬æ­¥éª¤ç”Ÿæˆç®€å•æŠ¥å‘Š"""
        try:
            import json
            
            # åˆ›å»ºreportç›®å½• - ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•è€Œä¸æ˜¯config.base_path
            current_dir = os.getcwd()
            report_dir = os.path.join(current_dir, "report")
            os.makedirs(report_dir, exist_ok=True)
            
            # ç”Ÿæˆæ­¥éª¤æŠ¥å‘Š
            step_report = {
                "step_name": step_name,
                "execution_time": datetime.now().isoformat(),
                "status": "completed",
                "result": str(result) if result else None,
                "config_info": {
                    "pressure_list": getattr(self.config, 'pressure_list', []),
                    "cfx_mode": getattr(self.config, 'cfx_mode', 'unknown'),
                    "base_path": getattr(self.config, 'base_path', 'unknown')
                }
            }
            
            # æ ¹æ®æ­¥éª¤ç±»å‹æ·»åŠ ç‰¹å®šä¿¡æ¯
            if step_name in ["generate_def", "generate_scripts"] and hasattr(result, '__len__'):
                step_report["generated_files_count"] = len(result) if result else 0
            
            report_file = os.path.join(
                report_dir,
                f"step_{step_name}_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(step_report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"æ­¥éª¤æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ­¥éª¤æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _cleanup_resources(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            if self.ssh_client:
                self.ssh_client.close()
                self.logger.debug("SSHè¿æ¥å·²å…³é—­")
        except Exception as e:
            self.logger.warning(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
    
    def execute_step_only(self, step_name: str, **kwargs) -> any:
        """
        åªæ‰§è¡ŒæŒ‡å®šæ­¥éª¤ï¼ˆç”¨äºè°ƒè¯•å’Œéƒ¨åˆ†æ‰§è¡Œï¼‰
        
        Args:
            step_name: æ­¥éª¤åç§°
            **kwargs: æ­¥éª¤å‚æ•°
            
        Returns:
            æ­¥éª¤æ‰§è¡Œç»“æœ
        """
        self.logger.info(f"æ‰§è¡Œå•ç‹¬æ­¥éª¤: {step_name}")
        
        result = None
        try:
            if step_name == "connect_server":
                result = self._connect_to_server()
            elif step_name == "verify_cfx":
                result = self._verify_cfx_environment()
            elif step_name == "generate_pre":
                job_configs = kwargs.get("job_configs", [])
                result = self.cfx_manager.generate_pre_files(job_configs)
            elif step_name == "generate_def":
                # å¯¹äºå•ç‹¬æ‰§è¡Œï¼Œéœ€è¦é‡æ–°ç”Ÿæˆpreæ–‡ä»¶å’Œjobé…ç½®
                job_configs = [
                    {
                        "pressure": pressure,
                        "job_name": f"{self.config.def_file_prefix}{pressure}",
                        "output_dir": f"{self.config.folder_prefix}{pressure}"
                    }
                    for pressure in self.config.pressure_list
                ]
                
                # è·å–é¢„ç”Ÿæˆçš„.preæ–‡ä»¶è·¯å¾„
                pre_file_path = os.path.join(
                    self.config.base_path, 
                    "create_def_batch.pre"
                )
                pre_files = [pre_file_path] if os.path.exists(pre_file_path) else []
                
                if not pre_files:
                    # å¦‚æœæ²¡æœ‰.preæ–‡ä»¶ï¼Œå…ˆç”Ÿæˆ
                    pre_files = self.cfx_manager.generate_pre_files(job_configs)
                
                result = self._generate_def_files(pre_files, job_configs)
            elif step_name == "generate_scripts":
                # ç”Ÿæˆä½œä¸šè„šæœ¬éœ€è¦å…ˆæœ‰.defæ–‡ä»¶å’Œjobé…ç½®
                job_configs = []
                
                for pressure in self.config.pressure_list:
                    job_config = {
                        "pressure": pressure,
                        "job_name": f"CFX_Job_{pressure}",
                        "output_dir": f"{self.config.folder_prefix}{pressure}",
                        "def_file": f"{self.config.folder_prefix}{pressure}/{pressure}.def"
                    }
                    
                    # å¦‚æœdef_file_prefixä¸ä¸ºç©ºï¼Œè°ƒæ•´defæ–‡ä»¶å
                    if self.config.def_file_prefix:
                        job_config["def_file"] = f"{self.config.folder_prefix}{pressure}/{self.config.def_file_prefix}{pressure}.def"
                    
                    # æ·»åŠ åˆå§‹æ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
                    if hasattr(self.config, 'initial_file') and self.config.initial_file:
                        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œåˆå§‹æ–‡ä»¶å°†åœ¨å¯¹åº”çš„P_Out_æ–‡ä»¶å¤¹ä¸­
                        initial_filename = os.path.basename(self.config.initial_file)
                        job_config["initial_file"] = initial_filename
                        self.logger.info(f"æ·»åŠ initial_fileåˆ°ä½œä¸šé…ç½®: {initial_filename} (æ¥æº: {self.config.initial_file})")
                    else:
                        self.logger.warning(f"æœªæ‰¾åˆ°initial_fileé…ç½®: hasattr={hasattr(self.config, 'initial_file')}, value={getattr(self.config, 'initial_file', 'NOT_FOUND')}")
                    
                    self.logger.info(f"å‹åŠ›{pressure}çš„ä½œä¸šé…ç½®: {job_config}")
                    job_configs.append(job_config)
                
                # å¦‚æœæ²¡æœ‰SSHè¿æ¥ä½†éœ€è¦é›†ç¾¤çŠ¶æ€ï¼Œå…ˆè·å–é›†ç¾¤çŠ¶æ€
                cluster_status = None
                if self.config.enable_node_detection and self.ssh_client:
                    try:
                        cluster_status = self.cluster_query.query_cluster_nodes(self.ssh_client)
                        self.logger.info(f"è·å–åˆ°é›†ç¾¤çŠ¶æ€ï¼ŒèŠ‚ç‚¹æ•°é‡: {len(cluster_status)}")
                    except Exception as e:
                        self.logger.warning(f"è·å–é›†ç¾¤çŠ¶æ€å¤±è´¥: {e}")
                
                result = self.script_generator.generate_job_scripts(job_configs, cluster_status)
            elif step_name == "upload_files":
                # ä¸Šä¼ æ–‡ä»¶åˆ°é›†ç¾¤
                if not self.ssh_client:
                    self._connect_to_server()
                
                # å‡†å¤‡è¦ä¸Šä¼ çš„æ–‡ä»¶å¤¹å’Œæ–‡ä»¶åˆ—è¡¨
                upload_items = []
                uploaded_folders = []
                uploaded_sh_files = []
                
                # æ·»åŠ æ¯ä¸ªå‹åŠ›å‚æ•°å¯¹åº”çš„å®Œæ•´æ–‡ä»¶å¤¹
                for pressure in self.config.pressure_list:
                    folder_name = f"{self.config.folder_prefix}{pressure}"
                    local_folder_path = os.path.join(self.config.base_path, folder_name)
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æ–‡ä»¶
                    if os.path.exists(local_folder_path) and os.path.isdir(local_folder_path):
                        folder_files = []
                        # æ·»åŠ æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
                        for root, dirs, files in os.walk(local_folder_path):
                            for file in files:
                                local_file_path = os.path.join(root, file)
                                # è®¡ç®—ç›¸å¯¹è·¯å¾„ä»¥ä¿æŒæ–‡ä»¶å¤¹ç»“æ„
                                rel_path = os.path.relpath(local_file_path, self.config.base_path)
                                upload_items.append(local_file_path)
                                folder_files.append(file)
                                self.logger.debug(f"æ·»åŠ æ–‡ä»¶åˆ°ä¸Šä¼ åˆ—è¡¨: {local_file_path} -> {rel_path}")
                        
                        if folder_files:
                            uploaded_folders.append({
                                "folder": folder_name,
                                "files": folder_files,
                                "file_count": len(folder_files)
                            })
                
                # æ·»åŠ ç”Ÿæˆçš„.shè„šæœ¬æ–‡ä»¶
                generated_sh_files = [
                    "Submit_All.sh",
                    "Monitor_Jobs.sh"
                ]
                
                for sh_file in generated_sh_files:
                    script_file = os.path.join(self.config.base_path, sh_file)
                    if os.path.exists(script_file):
                        upload_items.append(script_file)
                        uploaded_sh_files.append(sh_file)
                        self.logger.debug(f"æ·»åŠ ç”Ÿæˆçš„è„šæœ¬åˆ°ä¸Šä¼ åˆ—è¡¨: {script_file}")
                
                # æ·»åŠ æŒ‰ä½œä¸šå‘½åçš„.shæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                for pressure in self.config.pressure_list:
                    job_name = f"CFX_Job_{pressure}"
                    job_sh_file = os.path.join(self.config.base_path, f"{job_name}.sh")
                    if os.path.exists(job_sh_file):
                        upload_items.append(job_sh_file)
                        uploaded_sh_files.append(f"{job_name}.sh")
                        self.logger.debug(f"æ·»åŠ ä½œä¸šè„šæœ¬åˆ°ä¸Šä¼ åˆ—è¡¨: {job_sh_file}")
                
                # è¯¦ç»†è¾“å‡ºè¦ä¸Šä¼ çš„å†…å®¹
                self.logger.info(f"=== æ–‡ä»¶ä¸Šä¼ æ¸…å• ===")
                
                # è®¡ç®—åŸºæœ¬æ–‡ä»¶æ•°é‡å’Œå¤§å°
                total_size = 0
                total_file_count = len(upload_items)
                
                for item in upload_items:
                    if os.path.exists(item):
                        total_size += os.path.getsize(item)
                        
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆå§‹æ–‡ä»¶éœ€è¦é¢å¤–ä¸Šä¼ 
                initial_file_info = None
                if hasattr(self.config, 'initial_file') and self.config.initial_file:
                    if os.path.exists(self.config.initial_file):
                        initial_file_size = os.path.getsize(self.config.initial_file)
                        initial_file_size_mb = round(initial_file_size / (1024 * 1024), 2)
                        # ä¸ºæ¯ä¸ªP_Out_æ–‡ä»¶å¤¹éƒ½ä¼šé¢å¤–ä¸Šä¼ ä¸€ä»½åˆå§‹æ–‡ä»¶
                        additional_initial_files = len(self.config.pressure_list)
                        total_size += initial_file_size * additional_initial_files
                        total_file_count += additional_initial_files
                        initial_file_info = {
                            "name": os.path.basename(self.config.initial_file),
                            "size_mb": initial_file_size_mb,
                            "folders": additional_initial_files
                        }
                
                total_size_mb = round(total_size / (1024 * 1024), 2)
                self.logger.info(f"æ€»è®¡æ–‡ä»¶æ•°é‡: {total_file_count}")
                self.logger.info(f"æ€»è®¡æ–‡ä»¶å¤§å°: {total_size_mb} MB")
                
                # å¦‚æœæœ‰åˆå§‹æ–‡ä»¶ï¼Œå•ç‹¬è¯´æ˜
                if initial_file_info:
                    self.logger.info(f"  (åŒ…å«åˆå§‹æ–‡ä»¶ {initial_file_info['name']} Ã— {initial_file_info['folders']} = {initial_file_info['folders']}ä¸ªé¢å¤–æ–‡ä»¶)")
                
                # è¾“å‡ºæ–‡ä»¶å¤¹ä¿¡æ¯
                if uploaded_folders:
                    self.logger.info(f"è¦ä¸Šä¼ çš„æ–‡ä»¶å¤¹ ({len(uploaded_folders)}ä¸ª):")
                    for folder_info in uploaded_folders:
                        self.logger.info(f"  ğŸ“ {folder_info['folder']} ({folder_info['file_count']}ä¸ªæ–‡ä»¶)")
                        
                        # æ˜¾ç¤ºç°æœ‰æ–‡ä»¶
                        for file in folder_info['files']:
                            # è®¡ç®—æ–‡ä»¶å¤§å°
                            file_path = os.path.join(self.config.base_path, folder_info['folder'], file)
                            if os.path.exists(file_path):
                                file_size = os.path.getsize(file_path)
                                size_str = f"{round(file_size / 1024, 1)} KB" if file_size < 1024*1024 else f"{round(file_size / (1024*1024), 2)} MB"
                                file_type = "CFXå®šä¹‰æ–‡ä»¶" if file.endswith('.def') else "SLURMä½œä¸šè„šæœ¬" if file.endswith('.slurm') else "å…¶ä»–"
                                self.logger.info(f"     â””â”€â”€ {file} ({size_str}, {file_type})")
                            else:
                                self.logger.info(f"     â””â”€â”€ {file} (æ–‡ä»¶ä¸å­˜åœ¨)")
                                
                    # å•ç‹¬æ˜¾ç¤ºåˆå§‹æ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                    if initial_file_info:
                        self.logger.info(f"")
                        self.logger.info(f"é¢å¤–ä¸Šä¼ åˆå§‹æ–‡ä»¶åˆ°å„æ–‡ä»¶å¤¹:")
                        self.logger.info(f"  ğŸ“„ {initial_file_info['name']} ({initial_file_info['size_mb']} MB, CFXåˆå§‹æ–‡ä»¶)")
                        self.logger.info(f"     å°†å¤åˆ¶åˆ° {initial_file_info['folders']} ä¸ªP_Out_æ–‡ä»¶å¤¹ä¸­")
                else:
                    self.logger.warning("  âš ï¸  æ²¡æœ‰æ‰¾åˆ°è¦ä¸Šä¼ çš„æ–‡ä»¶å¤¹")
                
                # è¾“å‡º.shæ–‡ä»¶ä¿¡æ¯
                if uploaded_sh_files:
                    self.logger.info(f"è¦ä¸Šä¼ çš„.shè„šæœ¬æ–‡ä»¶ ({len(uploaded_sh_files)}ä¸ª):")
                    for sh_file in uploaded_sh_files:
                        script_path = os.path.join(self.config.base_path, sh_file)
                        if os.path.exists(script_path):
                            file_size = os.path.getsize(script_path)
                            size_str = f"{round(file_size / 1024, 1)} KB"
                            script_type = "æ‰¹é‡æäº¤è„šæœ¬" if "Submit" in sh_file else "ç›‘æ§è„šæœ¬" if "Monitor" in sh_file else "Shellè„šæœ¬"
                            self.logger.info(f"  ğŸ“œ {sh_file} ({size_str}, {script_type})")
                        else:
                            self.logger.info(f"  ğŸ“œ {sh_file} (æ–‡ä»¶ä¸å­˜åœ¨)")
                else:
                    self.logger.warning("  âš ï¸  æ²¡æœ‰æ‰¾åˆ°è¦ä¸Šä¼ çš„.shè„šæœ¬æ–‡ä»¶")
                
                self.logger.info(f"=== ä¸Šä¼ ç›®æ ‡ ===")
                self.logger.info(f"è¿œç¨‹ç›®å½•: {self.config.remote_base_path}")
                self.logger.info(f"ä¿æŒç›®å½•ç»“æ„: æ˜¯")
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ï¼Œè®°å½•è­¦å‘Š
                if not upload_items:
                    self.logger.warning("æ²¡æœ‰æ‰¾åˆ°è¦ä¸Šä¼ çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹")
                    result = {"uploaded_files": [], "failed_files": []}
                else:
                    # å…ˆåˆ›å»ºè¿œç¨‹ç›®å½•ç»“æ„
                    try:
                        # ç¡®ä¿è¿œç¨‹åŸºç¡€ç›®å½•å­˜åœ¨
                        stdin, stdout, stderr = self.ssh_client.exec_command(f"mkdir -p {self.config.remote_base_path}")
                        stdout.read()  # ç­‰å¾…å‘½ä»¤å®Œæˆ
                        self.logger.info(f"åˆ›å»ºè¿œç¨‹åŸºç¡€ç›®å½•: {self.config.remote_base_path}")
                        
                        # ä¸ºæ¯ä¸ªå‹åŠ›å‚æ•°åˆ›å»ºè¿œç¨‹ç›®å½•
                        for pressure in self.config.pressure_list:
                            folder_name = f"{self.config.folder_prefix}{pressure}"
                            remote_folder = f"{self.config.remote_base_path}/{folder_name}"
                            stdin, stdout, stderr = self.ssh_client.exec_command(f"mkdir -p {remote_folder}")
                            stdout.read()  # ç­‰å¾…å‘½ä»¤å®Œæˆ
                            self.logger.debug(f"åˆ›å»ºè¿œç¨‹ç›®å½•: {remote_folder}")
                            
                    except Exception as e:
                        self.logger.warning(f"åˆ›å»ºè¿œç¨‹ç›®å½•æ—¶å‡ºé”™: {e}")
                    
                    # æ‰§è¡Œä¸Šä¼ ï¼Œä¿æŒç›®å½•ç»“æ„
                    self.logger.info(f"å‡†å¤‡ä¸Šä¼ {len(upload_items)}ä¸ªæ–‡ä»¶åˆ°é›†ç¾¤ (ä¿æŒç›®å½•ç»“æ„)")
                    result = self.transfer_manager.upload_files(
                        ssh_client=self.ssh_client,
                        file_list=upload_items,
                        remote_dir=self.config.remote_base_path,
                        preserve_structure=True
                    )
                    
                    # å•ç‹¬å¤„ç†åˆå§‹æ–‡ä»¶ä¸Šä¼ åˆ°å„ä¸ªP_Out_æ–‡ä»¶å¤¹
                    if hasattr(self.config, 'initial_file') and self.config.initial_file:
                        initial_file_path = self.config.initial_file
                        if os.path.exists(initial_file_path):
                            self.logger.info(f"å¼€å§‹ä¸Šä¼ åˆå§‹æ–‡ä»¶åˆ°å„ä¸ªP_Out_æ–‡ä»¶å¤¹: {initial_file_path}")
                            
                            # æ„å»ºæ¨¡æ‹Ÿçš„defæ–‡ä»¶åˆ—è¡¨ç”¨äºåˆå§‹æ–‡ä»¶ä¸Šä¼ 
                            def_files_for_initial = []
                            for pressure in self.config.pressure_list:
                                folder_name = f"{self.config.folder_prefix}{pressure}"
                                local_folder_path = os.path.join(self.config.base_path, folder_name)
                                def_file = os.path.join(local_folder_path, f"{pressure}.def")
                                def_files_for_initial.append(def_file)
                            
                            self._upload_initial_files_to_folders(initial_file_path, def_files_for_initial)
                        else:
                            self.logger.warning(f"åˆå§‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ : {initial_file_path}")
            elif step_name == "submit_jobs":
                # æäº¤ä½œä¸šåˆ°é˜Ÿåˆ—
                if not self.ssh_client:
                    self._connect_to_server()
                
                # æ‰¾åˆ°Submit_All.shè„šæœ¬
                submit_script = os.path.join(self.config.base_path, "Submit_All.sh")
                if not os.path.exists(submit_script):
                    raise WorkflowError("Submit_All.shè„šæœ¬æœªæ‰¾åˆ°ï¼Œè¯·å…ˆæ‰§è¡Œgenerate_scriptsæ­¥éª¤")
                
                result = self._submit_jobs(submit_script)
            elif step_name == "monitor_jobs":
                # ç›‘æ§ä½œä¸šçŠ¶æ€
                if not self.ssh_client:
                    self._connect_to_server()
                
                # è·å–å·²æäº¤çš„ä½œä¸šä¿¡æ¯ï¼ˆä»ç”¨æˆ·ä½œä¸šé˜Ÿåˆ—æŸ¥è¯¢ï¼‰
                submitted_jobs = self._get_submitted_jobs()
                
                if not submitted_jobs:
                    self.logger.warning("æœªæ‰¾åˆ°æ­£åœ¨è¿è¡Œçš„ä½œä¸š")
                    result = {"status": "no_jobs", "jobs": []}
                else:
                    # æ‰§è¡Œç›‘æ§
                    result = self._monitor_jobs(submitted_jobs)
            elif step_name == "query_cluster":
                if not self.ssh_client:
                    self._connect_to_server()
                result = self.cluster_query.query_cluster_nodes(self.ssh_client)
            else:
                raise WorkflowError(f"æœªçŸ¥æ­¥éª¤: {step_name}")
            
            # ä¸ºå•ç‹¬æ­¥éª¤ç”Ÿæˆç®€å•æŠ¥å‘Š
            self.logger.info(f"æ­£åœ¨ä¸ºæ­¥éª¤ {step_name} ç”ŸæˆæŠ¥å‘Š...")
            self._generate_step_report(step_name, result)
            
            return result
                
        except Exception as e:
            self.logger.error(f"æ­¥éª¤æ‰§è¡Œå¤±è´¥ {step_name}: {e}")
            raise
        
        finally:
            # åªåœ¨ç‰¹å®šæ­¥éª¤åæ¸…ç†èµ„æº
            if step_name in ["connect_server", "query_cluster"]:
                # è¿™äº›æ­¥éª¤éœ€è¦ä¿æŒè¿æ¥ç»™åç»­æ­¥éª¤ä½¿ç”¨
                pass
            else:
                self._cleanup_resources()
    
    def get_execution_status(self) -> Dict:
        """è·å–å½“å‰æ‰§è¡ŒçŠ¶æ€"""
        return {
            "current_step": self.execution_state["current_step"],
            "completed_steps": self.execution_state["completed_steps"],
            "failed_steps": self.execution_state["failed_steps"],
            "progress": len(self.execution_state["completed_steps"]) / 10,  # å‡è®¾æ€»å…±10ä¸ªæ­¥éª¤
            "start_time": self.execution_state["start_time"],
            "total_jobs": self.execution_state["total_jobs"]
        }
